---
layout: post
title: 生成式大语言模型
---

我们下面介绍生成式的大语言模型。生成式大语言模型的工作方式和预训练大语言模型不一样：它总是根据前面的 Token 预测后面的 Token。这种工作方式有点像“前因后果”的意思，所以也常被称为“因果”模型。但这个“因果”并不是指的“因果关系”，而是指的“先后关系”，需要我们留意。

在大语言模型发展的第一阶段，大家比较热衷的是预训练大语言模型，不是生成式大语言模型。这是因为预训练大语言模型不把自己限制在“因果”上，也就是说：它也捕捉“后面的” Token 对“前面的” Token 的影响。因此，这样训练出来的模型，在各种下游任务上，性能就会比“局限在因果关系捕捉”上的生成式大语言模型要好。因此，这就比较出成果。

那时候，似乎只有 OpenAI 的科学家致力于生成式大语言模型，是因为他们的目标是 AGI：“通用人工智能”。具体来说，他们的目标是训练出一个模型，能够像人类那样根据前面的 Token 生成后面的 Token。这只有生成式大语言模型能够做到。

所以，OpenAI 对生成式大语言模型一直情有独钟。他们早期迷恋于用基于 RNN 的 Seq2seq 进行文本生成，为发现了控制文本生成的”情感“隐变量而欣喜。但是，大规模 RNN 的训练难度很大，导致其可扩展能力很弱，因此，他们很难对大规模的数据进行训练。

Google 发明的 Transformer 技术对 OpenAI 帮助极大。当 OpenAI 的科学家看到 Transformer 模型被用于 BERT，显示了极强的能力后，他们的眼睛一下子亮了，知道 Transformer 就是他们在寻找的东西。于是，他们立刻把 Transformer 用来做他们想做的生成式的语言模型。

这就出现了 GPT-1，GPT-2，直到 GPT-3（2020）。GPT-3 包括 175B 参数，在 45TB data 上进行了训练。当时，因为发现生成的文本几可乱真，所以 OpenAI 很担心这个模型会被人们用于生成各种假新闻等等的不合适的用途，因此对它的使用进行了控制，需要进行申请，获得授权后才能使用。

基于 GPT，OpenAI 进行了所谓的“对齐”工作，将 GPT 的生成结果和人类的需求进行“对齐”，就得到了 ChatGPT，开启了目前所有人都拭目以待的 AI 新时代。

### 课程材料

- Yandex 2023 LLM [PPT](https://github.com/yandexdataschool/nlp_course/tree/2023/week06_llm)

- 斯坦福 CS324 2022 年 [Capability](https://stanford-cs324.github.io/winter2022/lectures/capabilities/) 详细介绍了 GPT-3

- [LLM Visualization](https://bbycroft.net/llm), GPT-2, GPT-3 模型结构的可视化效果

- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)

- 斯坦福 CS224n NLG [PPT](https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture10-nlg.pdf) 

- 约翰霍普金斯 GA Lec 4 GPT PPT

- Yejin Choi, 2050: An AI Odyssey: Dark Matter of Intelligence, LxMLS 2023 PPT

## 代码

- Karpathy，nanoGPT: the simplest, fastest repository for training/finetuning medium-sized GPTs，[代码](https://github.com/karpathy/nanoGPT)

## 参考论文

普林斯顿推荐论文

GPT-3 (decoder-only models)
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/pdf/2005.14165.pdf)
- [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)

斯坦福 CS 224n NLG 推荐论文
- The Curious Case of Neural Text Degeneration
- Get To The Point: Summarization with Pointer-Generator Networks
- Hierarchical Neural Story Generation
- How NOT To Evaluate Your Dialogue System

## Andrej Karpathy 推荐论文

- Attention Is All You Need，[论文链接](https://arxiv.org/abs/1706.03762)，[Arxiv Dive](https://blog.oxen.ai/arxiv-dives-attention-is-all-you-need/)
    - Transformer 架构 —— 大语言模型采用的神经网络架构

- Language Models are Unsupervised Multitask Learners (GPT-2)，[论文链接](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)，[Arxiv Dive](https://blog.oxen.ai/arxiv-dives-language-models-are-unsupervised-multitask-learners-gpt-2/)
    - GPT-2 论文，如何通过无监督学习来预测下一个词，这种方法能够使模型学习到那些没有被明确训练的任务

- Sparks of Artificial General Intelligence: Early experiments with GPT-4, 22 Mar 2023, [PDF](https://arxiv.org/pdf/2303.12712.pdf)
    - 论文是微软对GPT-4早期版本的调查和研究。论文提出GPT-4可能是早期通用人工智能的火花

## Demo

- [GPT-J demo](https://6b.eleuther.ai)
- [OPT demo](https://opt.alpa.ai/#generation)
- [BLOOM demo](https://huggingface.co/bigscience/bloom)
- GPT-3 [Colab Demo](https://colab.research.google.com/drive/16WRWYYoulZrR0FLQqjoRNoxL7frtBZ1c?usp=sharing)
- OpenAI [Playground](https://platform.openai.com/playground)

| [Index](./) | [Previous](1-3-llm) | [Next](1-7-incontext)
