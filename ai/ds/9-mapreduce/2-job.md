---
layout: post
title: 职位分析
---

我们来分析 8 个最新的 Boss 直聘上的大数据开发方面的职位。咱们前面看过数据分析师的工作年薪最高能到 80 万，是吧？那么我们下面的大数据开发方法的工作，最高能到 120 万。

“大数据开发”的职位，是针对大数据的。下面是 2024 年 7 月的几个职位：

- [混沌学园数据仓库 15-30K·14薪](https://www.zhipin.com/job_detail/0747dd8445a4bc201n163d64ElpX.html)
- [好未来数据开发 25-50K·14薪](https://www.zhipin.com/job_detail/45c5999f7a3acc6203Z83dq5GVI~.html)
- [阿里巴巴数据开发 25-50K](https://www.zhipin.com/job_detail/cb642a256ee87e131nR-2tm8FVRU.html)
- [京东数据开发 25-50K·15薪](https://www.zhipin.com/job_detail/d71388ceb69a00cc1nN52tS5E1RZ.html)
- [快手数据开发 25-50K](https://www.zhipin.com/job_detail/1c0f0d92a8f7e03d1nFy29y9EVdV.html)
- [Shopee 数据 / 大数据工程师 50-80K·15薪](https://www.zhipin.com/job_detail/636cfbcbc1ed66801n162NS0FFZU.html)

我们来看看这些公司。首先，混沌学院为什么需要大数据工程师呢？我猜测是因为它是一个互联网的在线教育系统，所以它有很多教育视频、论坛、直播课等等。所以它需要有一个自己的大数据系统。好未来也类似：比如在疫情的时候，好未来的直播，有上千万的用户，所以它的大数据系统也很大。阿里巴巴、京东、快手、Shopee 分别是电商和视频，都是有海量的数据，因此需要大数据工程师。

从上面的数据可以看出，大数据工程师的最高工资是 Shopee 的，年薪能到 80*15 = 120 万，其它的和我们前面看过的数据分析师差不多。

大数据工程师和数据分析师的基本区别是：大数据工程师的工作更技术一点，更后台一点。一个不太准确，但形象的比喻是：数据分析师每天围着老板的办公室打转，而大数据工程师每天在机房实验室里打转。所以它们是两种不同的风格。大家可以想想自己适合哪种。你是想当老板的智囊团，贴身护卫呢，还是想管着 2 万台服务器，做它们的护卫呢？只要你钻研下去，成为专家，那么两者都很有成就感，都很赚钱。

好，我们下面来看职位的具体内容！

# 职位描述

首先看混沌学园数据仓库工程师的职位描述（[Boss 直聘网页](https://www.zhipin.com/job_detail/0747dd8445a4bc201n163d64ElpX.html)）。它的工资是 15-30K·14 薪，职位描述是：Python、Spark、Hadoop、Flink、Kafka、HDFS、Pandas、Java

我们首先看职位描述的关键字。其中第一个字 Python 和最后一个字 Java 可能是大家听过或者学过的，倒数第二个 Pandas 我们在上一节课也讲过了。但中间的 Spark、Hadoop、Flink、Kafka、HDFS 大家是第一次听说。它们就是我们这几节课的内容。我们下面对它们一一进行简单的介绍。

# Hadoop/HDFS

我们首先认识一下 Hadoop 和 HDFS。它首先是一个分布式的文件系统。因为大数据的数据特别大，一个电脑存不下，所以就需要存到几百台、几千台、甚至几万台电脑上。所以这种文件系统就是一个分布式的文件系统。Hadoop 上面的文件系统就叫 HDFS。HD 就是 Hadoop 的缩写，FS 就是文件系统（File System），所以 HDFS 就是 Hadoop 的文件系统。

除了 HDFS 存储数据，Hadoop 还支持 MapReduce 分布式计算。比如说我们交给 Hadoop 一个任务。这个任务需要访问存在几百台电脑上的文件。Hadoop 就会在所有存储相应文件的计算机上去启动一个计算任务，在每个电脑上做一点点计算工作，最后再把结果合起来。这种计算模式就叫 MapReduce。

# Spark

Hadoop 是基于硬盘存储的，Spark 把硬盘替换成了内存，因此大大地加快了运行速度。大家知道：如果我们计算机的内存太小的话，我们就会感觉到计算机运行速度很慢。这是因为我们不得不把很多东西存到硬盘上。但读硬盘是很慢的：我们经常在等电脑响应的时候，是不是会看到电脑的硬盘灯在拼命地闪？这就是它在拼命地读硬盘呢。所以读硬盘是很慢的。但是如果我们给电脑上加块 8G 或者 16G 内存，是不是就觉得电脑快很多？这就是因为我们的内存够了的话，很多东西就可以存到内存里，而内存的访问速度是硬盘访问速度的几百倍，那当然就快了。比如读个文件要一分钟，现在几十毫秒就读出来了，我们就觉得特别快。所以呢。Spark 就是对 Hadoop 做了类似的改进，把很多东西存到内存里去，这样计算起来就特别快。所以大家看上面的职位描述，Spark 是排在 Hadoop 之前的。为什么呢？因为 Spark 的速度快，所以工作里用 Spark 最多。但 Hadoop 是基础。

Spark 支持三种语言的编程。一种是 Scala。Spark 是用 Scala 写的。一种是 Java。Hadoop 是用 Java 写的。一种是 Python，因为 Python 最近越来越流行。 所以大家后面会看到一些工作职责中要求会这些语言。

Spark 的编程分为两种：一种是 RDD 编程。RDD 是弹性分布式数据集。这个“弹性”是什么意思？就是它会“反弹”。我们经常跟同学说：这个同学特别有弹性，什么意思呢？就是不论受到怎样的打击，他也会“弹回来”，然后继续工作和生活。因为他总会反弹。RDD 为什么能反弹呢？因为 Spark 的分布式存储，会将一个文件切成很多块。其中任何一块，都会在很多计算机上存。一旦其中哪台计算机坏了，其它计算机就会赶紧在另外的计算机上又存一块。这就是自我恢复，所以它是弹性的。然后，Spark 有一套机制，存着一个 RDD 是如何计算得来的路径，然后一旦计算中途出现了问题，比如某台计算机坏了，它就能按照这个路径从头开始，进行恢复。所以它是弹性的。

除了弹性的分布式数据集 RDD 编程，Spark 还支持类似 Pandas 的 DataFrame 编程。这就是前面为什么要请大家练习 Pandas 的原因，因为学习了它之后，在大数据的分析中也很有用。

Spark 的功能非常强大。它里面有很多很多的库，比如机器学习算法：聚类、分类、推荐。所以在公司里用得特别多。

# Kafka

Kafka 是大数据系统里的数据总线。大家知道电脑里面有一条数据总线，对吧？数据总线就像高铁似的。高铁就是我们中国交通的一个总线：大家出去旅行了，都是先坐公交车坐到高铁站。然后就汇入了一个总线，就开始到处乱跑了，对吧？Kafka 也是那样：大数据系统中的各个组件之间传递消息，都是先给 Kafka，然后 Kafka 查一下谁需要这些消息，就给运送过去。所以它起到一个数据总线的作用。如果大数据系统没有 Kafka，那就像我们中国没有了高铁。所以这个岗位要求里要求掌握 Kafka，因为数据总线把整个公司的大数据设备连起来。

# Flink

Flink 是针对流处理的。流处理就像一条河流一样，来一点数据就做实时的处理，因此反应非常敏捷，非常适合实时分析，因此也非常流行。

# 工作职责

好，下面我们来看它的工作职责。具体内容如下：

工作职责:
1. 负责离线/实时大数据存储/计算平台的设计开发和维护工作；
2. 负责设计基于大数据的各种计算，etl流程设计和开发；
3. 通过设计运用合适的技术方案来支撑各种业务场景：推荐，转化，增长，商业化等；
4. 负责新技术的研究和搭建。

第一是上面这些大数据平台的设计、开发和维护。这和数据分析的工作职责就完全不一样了。设计和开发这两项工作很好理解。维护也是一项重要的工作，就是我们得保证它平稳、高效地工作。比如前几天的蓝屏事件导致很多航班中止，这就是系统开发导致的系统维护问题。系统崩溃了，老板急得直冒火，一定会找你：赶紧恢复，我限你 1 小时内恢复，否则你走人，是这样的吧？

第二是计算流程的设计和开发。这是因为公司里很多大数据的分析工作是常规动作，比如淘宝想统计一下昨天我卖了多少鞋。那这种计算流程我们就得给开发好了，供业务部门用。最后还画个大屏，给实时展示一下。等等。

第三是运用技术来支撑业务场景，包括推荐、增长。比如说公司今年双 11 要搞个活动，弄好友推荐。老板说我现在有 3 万用户。如果每个用户给我拉 10 个朋友过来，那么用户数可能就涨到 30 万了。那我们的平台原来是 300 个节点，现在是不是要扩充到 3000 个节点？那我们是不是要在双 11 前都搞好？所以就是为业务部门服务，支持各种业务场景。

第四是新技术的研究和搭建。这就是要系统升级，新系统可行性分析和部署了。

所以，上面这些大数据工程师的活，跟我们前面讨论过的数据分析师的活，有很大区别。它更偏后端，更支持性一点。

# 工作心态

那么，我们以一种什么心态来对待这些工作呢？

第一是主动。大家注意到没有，这些公司的职位一般都要求你有“主动解决问题的素质”。所以，第一是“主动”。比如：要主动向人问，本来也是大家一起开开心心的，弄一个事，对不对？然后，主动问 GPT。我们现在可以充分地利用 ChatGPT，人工智能也可以帮大家很多的忙。最后，是主动参加技术社区的活动。因为大数据的平台一直在不断地变化、升级。唯一不变的就是变化。所以今天你把它装好了，明天升级后，可能就不行了，所以这时候我们就得主动排查问题。这都正常。

第二是快乐。大家要把排查问题当做一种快乐，就能成为大数据计算专家。为什么呢？大家想想，咱们拿到这么高的工资，年薪百万，不就是解决问题么？难道别人年薪百万请我们来，就是让我们过来喝茶？当然不可能，是不是？所以有问题，反而是我们的机会。应该感到有一种激动，有一点兴奋，有一种内在的冲动：因为英雄终于有用武之地了。带着这样的一种快乐去去排查问题，就会特别开心。大家知道：好的精神状态，对身体健康有很大好处，而锻炼身体，也可以让精神更好。我们每天斗志昂扬地，为了年薪百万，目标明确地战斗。生活是很艰难。虽然艰难，但是也很开心，是不是？

为什么我要特意叮嘱一下工作心态呢？因为在大数据平台这一块，很多同学很容易有挫折感，而且在工作岗位上，一旦出了问题，就要 24 小时工作，排查问题，压力很大。所以我们需要在挫折和压力下保持一个良好的心态。陈老师也经历过无数的挫折。这都是正常的。大家看看以前学长的实验记录，就会发现每个同学都走过了无数的坑。学长们跟大家一样，在上这门课之前，都是小白，没有谁学过大数据的。比如这位同学，她遇到的第一个困难是怎么用 vi 打开一个文件，然后是编辑完了之后怎么保存？刚开始都是一头雾水。这些都正常。每个同学都遇到了无数的困难。

# 学习方法

大数据平台的特点是东西多、内容散，上课讲的主要是概念。大家看这些视频，每个视频也就几分钟、十几分钟。最长的这个 Paxos 算法比较复杂，23 分钟也讲完了。但这不是全部。因为咱们不可能深入到里头去给大家讲里面的代码实现，所以都只是在应用的层面上讲一些概念。但实际工作中肯定不是这样：有大量的细节。我们管着一个特别大的平台。这个平台上面有很多人在上面工作。我们要负责维护这个平台，升级、安装、设计、开发。是这种感觉。

所以大数据平台的学习方法就是课下大家多练。只要多练，就能成功。有很多同学毕业后从事这方面的工作。我有个研究生，她研一学了这个课以后，到研三毕业找工作的时候，又自己复习了一遍，因为发现面试的时候，常被问到。最后她去了银行还是金融的一个公司，就管公司的大数据系统。

# 任职资格

我们下面看岗位的具体要求，即任职资格，如下：

1. 三年以上相关工作经验；
2. 熟练掌握Linux操作系统，精通Java/Python语言的一种或多种；
3. 熟悉主流大数据工具Hadoop、spark、flink、ELK中两个及以上,并熟悉所使用工具的技术原理、主要特点；
4. 熟练使用flume、Kafka、hbase、redis、mysql等常用工具；
5. 对数仓建模有较深的理解
6. 有Flink的实时数仓开发经验优先；
7. 有OLAP大数据平台开发经验者优先。

首先看第一条：三年以上相关工作经验。大家现在研一上这个课，做练习，就是工作经验了。然后三年以后毕业，正好三年。所以这一点没有问题。

第二条，熟练掌握 Linux 系统。咱们 AI 编程环境里的系统就是 Linux。精通 Java/Python 语言中的一种。注意，其实懂一种就够了。比如我们懂 Python 就好了。有的同学问：要不要专门去学一个 Java。我的建议是别学了。研究生应该干研究生的活。比如把大数据的原理和系统给整明白了，把论文认真地写好了，多发几篇小论文，没错吧？我们会的东西，要别人不会，才有意义。我们去学 Java，拿了一个证，回头一看，中国会 Java 的已经有 5000 万了。那我们的简历里写这个和不写这个，有什么区别？我们得写自己的突出的特点，对不对？很多人说：学 Java 找工作容易。为什么找工作容易？是因为 Java 的职位多。但它职位多，会的人也多，那我们有什么优势？人家简历上要的是你设计了一个什么，实现了一个什么，获得了多少利润，要的是这些东西对不对？你会十门语言都没有用，所以就会一种就行。

第三条是熟练这些工具的使用和原理、特点。这是我们要掌握的。面试的时候要经得住问。

最后是有什么经验的优先。这其实意味着你不会这个，但前面的满足要求，也不影响你得到这个职位。“优先”是意味着我们没有这个也没问题；“精通”就是说用它真的开发过东西，做出过产品，所以我们要自己给自己布置高难度的练习；“熟悉”就是说我能够把它用起来，对它聊得上来。因为现在有 AI，所以这些其实都没有问题。关键是什么呢？关键就是我们真的干过啥？就是这个。有没有案例？有没有成功经验？解决了什么问题？最后效果怎么样？和我们写论文差不多。

基于上面的讨论，我们对下面这些职位的理解，就比较容易了。

# 好未来数据开发

25-50K·14薪

职位描述

Hive、Hadoop

任职资格：
1. 3 年以上相关工作经验，计算机相关专业本科以上学历
2. 熟悉 Hadoop、Hive、Hase、Spark 等大数据处理工具，熟悉 Hadoop、Spark 原理和机制
3. 熟悉Python语言编程，有基于Hive、Spark等开发经验，熟悉scala、java语言优先
4. 熟悉数据仓库设计与实现，有过相关实践经验者优先
5. 有良好而逻辑思维，喜欢底层数据开发，对业务数据敏感
6. 做事积极主动、有良好的沟通和合作能力 

工作职责

负责新项目中数据开发相关工作，具体如下
1. 业务数据仓库建设中数据提取、清洗等
1. 业务数据BI指标开发
1. 业务分析的数据开发支持

[网页](https://www.zhipin.com/job_detail/45c5999f7a3acc6203Z83dq5GVI~.html)

好未来的职位描述中提到 Hive。Hive 是 Hadoop 上面的一个可以执行 SQL 查询的环境。因为很多同学会 SQL，那么大家就想最好在大数据上也用 SQL，这样就省了 Python/Java 编程了。所以，大家就开发了 HIVE。

这个职位的工作职责非常清楚，就是用大数据平台做基本的数据整理、清洗、业务指标开发。我猜是面向学生续费这样的商业分析。它对平台维护部分的要求不高，主要是要求我们能用它来编程，实现业务需求。所以它要我们喜欢数据开发、对业务数据敏感。它也要求我们做事积极主动，有良好的沟通能力，这些和我们做数据分析师的要求有一些一致，因为我们要理解业务需求，然后进行具体的技术实现。这里面有大量的沟通工作。不主动不行。

# 阿里巴巴数据开发

25-50K

职位描述

数据开发、数据挖掘、数据分析、Python、Java、Shell、Perl、实时

岗位描述：
1. 支持阿里妈妈全业务线(包括电商营销/品牌营销/DMP等)，服务广告主端的基础数据开发。
2. 营销分析平台开发 。包括人群洞察平台、价值模型设计、效果归因模型、归因引擎系统研发。
3. 面向顶级客户, 提供领先的营销数据分析全局解决方案.

岗位要求：
1. 2年以上工作经验，计算机、统计学、数学相关专业本科以上学历 。
2. 精通常见分布式数据系统（Hadoop/Storm）、主流数据库的原理及开发。
3. 熟练掌握一种高级语言（Java/C++）和一种脚本语言（shell/python/perl） 。
4. 掌握数据仓库建模技术，或常用数据挖掘技术，具有良好的统计学基础。 能够综合运用解决实际问题。
5. 优秀的数据敏感性和业务理解能力。
6. 有大型分布式系统设计与应用经验、或营销分析领域深入研究者优先。

[网页](https://www.zhipin.com/job_detail/cb642a256ee87e131nR-2tm8FVRU.html)

阿里的这个和好未来差不多，但更系统一些，要求进行阿里妈妈的营销业务模型、引擎开发。所以它是要在大数据的基础上，再做一次开发，得到一个自己的独特的平台。相比较而言，混沌学园和好未来主要是“用”。这也互联网公司的特点，它们喜欢做一个中台，开发一些工具给整个阿里用。所以，到这些公司去，相当于是加入一个开发团队，去开发一套系统。因此它要求数据库的开发经验，要求掌握 Java 或者 C++。这些都是开发系统需要的。

# 京东数据开发

25-50K·15薪

职位描述

Python、Kafka、MapReduce、Flume、Hive、Flink

岗位职责
1. 主导京东数据中台公共数据服务层搭建与推广，赋能业务部门与品牌商。
2. 负责京东数据中台离线和实时模型体系的设计与开发工作。
3. 对接京东商城业务部门进行数据需求沟通及整合落地工作。

# 京东数据开发

任职要求
1. 有大数据分布式计算平台开发经验，熟悉MapReduce、spark原理，熟悉使用Python、hive和spark开发。
2. 熟悉数据仓库各类建模理论，以及数据仓库数据层级关系，精通多维数据模型设计，具备大型数据仓库架构设计、模型设计和处理性能调优等相关经验。
3. 工作认真、负责、仔细，有良好的团队合作精神，良好的分析能力、沟通技巧。
4. 熟悉ES、Presto、CK等大数据AD HOC平台优先。
5. 有实时数据仓库开发经验优先。
6. 有3年以上数据仓库开发经验，互联网/电商行业优先。

[网页](https://www.zhipin.com/job_detail/d71388ceb69a00cc1nN52tS5E1RZ.html)

京东这个就是直接要求建立数据中台了。类似于阿里，它也要求数据仓库的建模、开发。

# 快手数据开发

25-50K

职位描述
HDFS

分布式存储研发工程师/专家-【数据架构】
职位描述：
1. 参与快手EB级分布式文件系统、对象存储和KV系统的研发工作，重点关注可用性、可靠性、扩展性、成本、性能和可运维性，支持好公司内部各项业务的存储需求
2. 负责需求的抽象、技术方案的设计和实现，推进各项功能的高质量落地
3. 负责线上稳定性问题的主动发现、分析定位和解决

职位要求：
1. 熟悉存储系统、分布式算法、计算机网络，理解大型分布式存储系统工作原理
2. 精通Java或C++，熟悉多线程编程、网络编程，熟悉常见的数据结构和算法
3. 具备优秀的逻辑思维能力，问题抽象能力和工程实现能力
4. 对技术有兴趣，自驱力强，对于产出质量有很高的自我要求
5. 熟悉HDFS、HBase、Ceph、Cassandra、Rocksdb等存储系统者尤佳

[网页](https://www.zhipin.com/job_detail/1c0f0d92a8f7e03d1nFy29y9EVdV.html)

快手的这个职位就更底层一点了：它是一个分布式文件存储系统的研发。它应该是基于开源系统，部署了一套 EB 级的分布式系统。快手是视频系统。视频动不动就几个 G，所以它要弄一个大数据的分布式存储系统。

# Shopee 数据 / 大数据工程师

50-80K·15薪

职位描述

Python、Spark、HIVE、Hadoop、Hbase、Kafka、Presto、clickhouse

岗位职责：

负责公司的各个大数据组件的日志分析，为数据运营以及业务部门提供决策支持，驱动集群资源利用率和用户任务的执行效率的提升。

岗位要求：
1. 统招本科及以上学历，计算机等理工科相关专业，3年以上工作经验；
2. 熟悉数据仓库理论，具备企业级数据仓库开发经验，日增量数据达到TB级；
3. 熟练掌握SQL开发，熟练掌握SQL的执行过程和优化方法；
4. 熟练掌握Hadoop、Hive、HBase、Spark、Presto、ClickHouse等大数据开发工具中一种或几种；
5. 熟悉Linux系统，具备shell、python等脚本和Java语言开发能力。

[网页](https://www.zhipin.com/job_detail/636cfbcbc1ed66801n162NS0FFZU.html)

Shopee 的岗位职责也主要是面向业务。但它的工资高，可能是因为它是一家外资企业。外资企业为什么工资高呢？因为你得有一门重要的技能：英语口语交流能力。所以大家要高度重视英语的口语交流能力。这个能力是非常值钱的。

看到这里，大家有没有感觉到：大数据平台的工作，看起来比数据分析的工作，技术含量高一些，但为什么它们挣的钱其实差不太多呢？这是因为数据分析是围着老板转的，直接影响公司经营决策，也就是说和公司是否赚钱，直接相关。因为它是赚钱的，所以它分钱就不会少。大数据平台的工作呢？是支持性的工作，在公司里属于成本单位，是要靠老板给你拨钱的，所以老板肯定会控制成本。总之，我们在公司里，要尽量去赚钱的部门，因为离钱越近，就越挣钱。比如做数据分析，我们稍微帮老板决策提供一个支持，让她多挣了一个亿，那么，靠谱的老板，给你分个 100 万，也是小意思，对吧。就是这个道理。但不是每个人都喜欢和老板打交道的，很多人喜欢和机器打交道，那我们就安心做大数据工程师，也是非常优秀的。每个人找到自己喜欢的职业就好，就都会成功。

上面这个职业分析花了一点时间给大家讲，主要是让大家确定好目标。因为我相信大家都是研究生了，只要目标设定好了，大家考研这么难的都搞定了，还害怕一个 Linux 学不会？不可能的。大家只要明白了做一个事情的意义，目标明确，就没有学不会的东西。我相信大家这一点。

<br/>

|[Index](../) | [Previous](1-bigdata) | [Next](3-hdfs)|
